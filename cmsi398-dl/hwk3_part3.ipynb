{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HWK 3 -- part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Make a linear module (call it mod) with the following weight matrix and bias vector:\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "1 &1 \\\\ 2 & 0 \\\\ -1 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "b = \\begin{bmatrix}\n",
    "1  \\\\ 1 \\\\ -1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "(Hint: the weights and bias are going to be initialized randomly, then you will need to change them one by one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [-1.,  1.]], grad_fn=<CopySlices>)\n",
      "Parameter containing:\n",
      "tensor([ 1.,  1., -1.], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "mod = nn.Linear(2,3,bias=True)\n",
    "mod.weight[0,0] = 1\n",
    "mod.weight[0,1] = 1\n",
    "mod.weight[1,0] = 2\n",
    "mod.weight[1,1] = 0\n",
    "mod.weight[2,0] = -1\n",
    "mod.weight[2,1] = 1\n",
    "mod.bias[0] = 1\n",
    "mod.bias[1] = 1\n",
    "mod.bias[2] = -1\n",
    "print(mod.weight)\n",
    "print(mod.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Make a vector $x = \\begin{bmatrix} 2  \\\\ 1  \\end{bmatrix} $  and feed it to the module. Print the output. \n",
    "(Hint: make sure to use torch.Tensor with a capital T! If you use torch.tensor you will get a bug. Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  5., -2.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2,1])\n",
    "y = mod(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check with pen and paper that pytorch got the correct answer. (I will collect it, so write it clearly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Make a 3 layer neural net as follow:\n",
    "$$\n",
    "\\text{Linear layer 1} \\to  \\text{ReLU} \\to \\text{Linear layer 2} \\to  \\text{ReLU} \\to \\text{Linear layer 3}\n",
    "$$\n",
    "### Set the weight matrices of the three linear layers to be \n",
    "$$\n",
    "W_1 \n",
    "= \\begin{bmatrix}\n",
    "1 &1 \\\\ 0 & -1 \n",
    "\\end{bmatrix} \\qquad \\text{,} \\qquad\n",
    "W_2 \n",
    "= \\begin{bmatrix}\n",
    "-1 &1 \\\\ 1 & -1 \n",
    "\\end{bmatrix}\n",
    "\\qquad \\text{and} \\qquad\n",
    "W_3 \n",
    "= \\begin{bmatrix}\n",
    "0 &2 \\\\ 1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "### Let's not put any bias. Put a ReLU nonlinearity between the linear layers, but do NOT put any softmax at the end (so the network will output raw scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class three_layer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear( input_size, hidden_size , bias=False)\n",
    "        self.layer2 = nn.Linear( hidden_size, output_size , bias=False)   \n",
    "        self.layer3 = nn.Linear( hidden_size, output_size , bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = three_layer_net(2,2,2)\n",
    "net.layer1.weight[0,0] = 1\n",
    "net.layer1.weight[0,1] = 1\n",
    "net.layer1.weight[1,0] = 0\n",
    "net.layer1.weight[1,1] = -1\n",
    "net.layer2.weight[0,0] = -1\n",
    "net.layer2.weight[0,1] = 1\n",
    "net.layer2.weight[1,0] = 1\n",
    "net.layer2.weight[1,1] = -1\n",
    "net.layer3.weight[0,0] = 0\n",
    "net.layer3.weight[0,1] = 2\n",
    "net.layer3.weight[1,0] = 1\n",
    "net.layer3.weight[1,1] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Make a vector $x = \\begin{bmatrix} 2  \\\\ 1  \\end{bmatrix} $  and feed it to the network. Print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 3.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([2,1])\n",
    "y = net.forward(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Check with pen and paper that pytorch got the correct answer. (I will collect it, so write it carefully)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Use Pytorch to compute the softmax of the following vectors:\n",
    "$$\n",
    "x = \\begin{bmatrix} 1  \\\\ 2 \\\\-2  \\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2654, 0.7214, 0.0132])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1,2,-2])\n",
    "p = torch.softmax(x,dim=0)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Check with simple python command that pytorch correctly computed the softmax. Follow the steps below (note that you need to import the package \"math\" in order to have access to the exponential function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "7.38905609893065\n",
      "0.1353352832366127\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# compute the exponential of the thre entries of the vector:\n",
    "y1=math.exp(1)\n",
    "y2= math.exp(2)\n",
    "y3= math.exp(-2)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26538792877224193\n",
      "0.7213991842739688\n",
      "0.013212886953789417\n"
     ]
    }
   ],
   "source": [
    "# then divide by the sum and you should recover the values computed by pytorch\n",
    "p1= y1/(y1+y2+y3)\n",
    "p2= y2/(y1+y2+y3)\n",
    "p3= y3/(y1+y2+y3)\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Create the matrix\n",
    "$$\n",
    "x = \\begin{bmatrix} 1  & 2 & -2  \\\\ 1  & 5 & -2  \\end{bmatrix} \n",
    "$$\n",
    "### Note that the first row is the vector from part a). Compute the softmax of this matrix in the dimension 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6539e-01, 7.2140e-01, 1.3213e-02],\n",
      "        [1.7970e-02, 9.8114e-01, 8.9468e-04]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,-2],[1,5,-2]])\n",
    "p = torch.softmax(x, dim = 1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Explain in the cell below why the result you got make sense. In order to write in a cell (rather than puting some code, you need to switch from \"code\" to \"Markdown\" in the box right below the Widget and Help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the result makes sense because we are working in dimension one which takes the softmax by the rows independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now compute the softmax in the dimension 0, and explain why it makes sense (in particular, explain the 50% probabilities in the first and last columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.0474, 0.5000],\n",
      "        [0.5000, 0.9526, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "p = torch.softmax(x, dim=0)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this result makes sense because it is in dimension 0 so it takes the softmax by the column. It is 50% in the first and third column because the entries in the matrix are the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
