{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_2_Feature_S(object):\n",
    "    # converts from df to list of features separated by sentence\n",
    "    def __init__(self, text):\n",
    "        self.num = 1\n",
    "        func = lambda f: [(word, pos) for word, pos in zip(f['Word'].values.tolist(), f['POS'].values.tolist())]\n",
    "        self.group = text.groupby('Sentence #').apply(func)\n",
    "        self.sents = [f for f in self.group]\n",
    "\n",
    "    def get_next(self):\n",
    "        f = self.group['Sentence: {}'.format(self.num)]\n",
    "        self.num += 1\n",
    "        return f\n",
    "        \n",
    "class DF_2_Target_S(object):\n",
    "    # converts from df to list of labels separated by sentence\n",
    "    def __init__(self, text):\n",
    "        self.num = 1\n",
    "        func = lambda l: [(tag) for tag in l['Tag'].values.tolist()]\n",
    "        self.group = text.groupby('Sentence #').apply(func)\n",
    "        self.sents = [l for l in self.group]\n",
    "\n",
    "    def get_next(self):\n",
    "        l = self.group['Sentence: {}'.format(self.num)]\n",
    "        self.num += 1\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF(nn.Module):\n",
    "    def __init__(self, num_words, emb_dim, num_y):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_words, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, num_y)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeds = self.emb(text)\n",
    "        return self.softmax(self.linear(embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(text):\n",
    "    feat_to_idx = {}\n",
    "    for sent in text:\n",
    "        for feat in sent:\n",
    "            feat_to_idx.setdefault(feat, len(feat_to_idx))\n",
    "    return feat_to_idx\n",
    "\n",
    "def load_tag(text):\n",
    "    tag_to_idx = {}\n",
    "    for sent in text:\n",
    "        for tag in sent:\n",
    "            tag_to_idx.setdefault(tag, len(tag_to_idx))\n",
    "    return tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "train_data = pd.read_csv('trainDataWithPOS.csv')\n",
    "test_data = pd.read_csv('testDatawithPOS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "train_sent_feature = DF_2_Feature_S(train_data).sents\n",
    "train_sent_label = DF_2_Target_S(train_data).sents\n",
    "x_test = DF_2_Feature_S(test_data).sents\n",
    "y_test = DF_2_Target_S(test_data).sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 80% train and 20% validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_sent_feature, train_sent_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The features used are the word and the POS')\n",
    "\n",
    "# convert features and labels to numbers\n",
    "feat_to_idx = load_vocab(x_train)\n",
    "feat_to_idx[('UNK')] = len(feat_to_idx)-1\n",
    "tag_to_idx = load_tag(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes the features from tuples to strings\n",
    "x_train_ = []\n",
    "x_val_ = []\n",
    "x_test_ = []\n",
    "old_x_data = [x_train, x_val, x_test]\n",
    "new_x_data = [x_train_, x_val_, x_test_]\n",
    "\n",
    "for i in range(len(old_x_data)):\n",
    "    for sentence in old_x_data[i]:\n",
    "        new_sent = []\n",
    "        for feat in sentence:\n",
    "            new_x_data[i].append(str(feat))\n",
    "\n",
    "# changes the tags from tuples to strings of indices\n",
    "y_train_ = []\n",
    "y_val_ = []\n",
    "y_test_ = []\n",
    "old_y_data = [y_train, y_val, y_test]\n",
    "new_y_data = [y_train_, y_val_, y_test_]\n",
    "\n",
    "for i in range(len(old_y_data)):\n",
    "    for sentence in old_y_data[i]:\n",
    "        new_sent = []\n",
    "        for tag in sentence:\n",
    "            tag_idx = tag_to_idx[tag]\n",
    "            new_y_data[i].append(str(tag_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "train_data = cv.fit_transform(x_train_)\n",
    "val_data = cv.transform(x_val_)\n",
    "test_data = cv.transform(x_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Accuracy: 0.9089280268374504\n",
      "Naive Bayes Validation Accuracy: 0.903395061728395\n",
      "Naive Bayes Test Accuracy: 0.897811059907834\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(train_data, np.array(y_train_).flatten())\n",
    "MNB_train_acc = MNB.score(train_data, np.array(y_train_).flatten())\n",
    "MNB_val_acc = MNB.score(val_data, np.array(y_val_).flatten())\n",
    "MNB_test_acc = MNB.score(test_data, np.array(y_test_).flatten())\n",
    "print('Naive Bayes Training Accuracy:', MNB_train_acc)\n",
    "print('Naive Bayes Validation Accuracy:', MNB_val_acc)\n",
    "print('Naive Bayes Test Accuracy:', MNB_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.9200594693504117\n",
      "Logistic Regression Validation Accuracy: 0.9080246913580247\n",
      "Logistic Regression Test Accuracy: 0.9024193548387097\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       615\n",
      "           1       0.92      0.98      0.95      6722\n",
      "           2       0.83      0.78      0.80       831\n",
      "           3       0.84      0.32      0.46       253\n",
      "           4       0.90      0.14      0.24       259\n",
      "\n",
      "    accuracy                           0.90      8680\n",
      "   macro avg       0.86      0.60      0.65      8680\n",
      "weighted avg       0.90      0.90      0.89      8680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(max_iter=1500)\n",
    "LR.fit(train_data, np.array(y_train_).flatten())\n",
    "LR_train_acc = LR.score(train_data, np.array(y_train_).flatten())\n",
    "LR_val_acc = LR.score(val_data, np.array(y_val_).flatten())\n",
    "LR_test_acc = LR.score(test_data, np.array(y_test_).flatten())\n",
    "print('Logistic Regression Train Accuracy:', LR_train_acc)\n",
    "print('Logistic Regression Validation Accuracy:', LR_val_acc)\n",
    "print('Logistic Regression Test Accuracy:', LR_test_acc)\n",
    "\n",
    "preds = LR.predict(test_data)\n",
    "report = classification_report(np.array(y_test_).flatten(), preds)\n",
    "print('\\n')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 30\n",
    "learning_rate = 0.5\n",
    "model = FF(len(feat_to_idx), emb_dim, len(tag_to_idx))\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "print('Feed Forward Network')\n",
    "print('Hyperparameters: emb_dim = 30, lr = 0.5, optim = SGD, loss = NLLL, epochs = 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Training loss: 0.2884339690208435\n",
      "\n",
      "Epoch: 1\n",
      "Training loss: 0.21702352166175842\n",
      "\n",
      "Epoch: 2\n",
      "Training loss: 0.20057250559329987\n",
      "\n",
      "Epoch: 3\n",
      "Training loss: 0.20221562683582306\n",
      "\n",
      "Epoch: 4\n",
      "Training loss: 0.20850208401679993\n",
      "\n",
      "Epoch: 5\n",
      "Training loss: 0.21509265899658203\n",
      "\n",
      "Epoch: 6\n",
      "Training loss: 0.22058093547821045\n",
      "\n",
      "Epoch: 7\n",
      "Training loss: 0.2246812880039215\n",
      "\n",
      "Epoch: 8\n",
      "Training loss: 0.2272353321313858\n",
      "\n",
      "Epoch: 9\n",
      "Training loss: 0.22829210758209229\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i in range(len(x_train)):\n",
    "        x = [feat_to_idx[feat] for feat in x_train[i]]\n",
    "        y = [tag_to_idx[tag] for tag in y_train[i]]\n",
    "        x_train_tensor = torch.LongTensor(x)\n",
    "        y_train_tensor = torch.LongTensor(y)\n",
    "        pred_y = model(x_train_tensor)\n",
    "        loss = loss_fn(pred_y, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(\"\\nEpoch:\", epoch)\n",
    "    print(\"Training loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Network Validation Accuracy: 0.8891975308641975\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for i in range(len(x_val)):\n",
    "        x = [feat_to_idx.get(feat, len(feat_to_idx)-1) for feat in x_val[i]]\n",
    "        y = [tag_to_idx[tag] for tag in y_val[i]]\n",
    "        x_val_tensor = torch.LongTensor(x)\n",
    "        y_val_tensor = torch.LongTensor(y)\n",
    "        pred_y_val = model(x_val_tensor)\n",
    "        pred_y_val = torch.argmax(pred_y_val, dim=1)\n",
    "        num_correct = 0\n",
    "        for j in range(len(pred_y_val)):\n",
    "            num_correct += (pred_y_val[j] == y_val_tensor[j]).sum().item()\n",
    "        correct += num_correct\n",
    "    print('Feed Forward Network Validation Accuracy:', correct/len(x_val_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward Network Test Accuracy: 0.8808755760368664\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for i in range(len(x_test)):\n",
    "        x = [feat_to_idx.get(feat, len(feat_to_idx)-1) for feat in x_test[i]]\n",
    "        y = [tag_to_idx[tag] for tag in y_test[i]]\n",
    "        x_test_tensor = torch.LongTensor(x)\n",
    "        y_test_tensor = torch.LongTensor(y)\n",
    "        pred_y_test = model(x_test_tensor)\n",
    "        pred_y_test = torch.argmax(pred_y_test, dim=1)\n",
    "        num_correct = 0\n",
    "        for j in range(len(pred_y_test)):\n",
    "            num_correct += (pred_y_test[j] == y_test_tensor[j]).sum().item()\n",
    "        correct += num_correct\n",
    "    print('Feed Forward Network Test Accuracy:', correct/len(x_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('results not far off from naive bayes and logistic regression')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
