{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oFDvAnYI0QXH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCD93X7iCdQc"
   },
   "source": [
    "Build the feed-forward neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "M7PMbNjO0dOl"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self, num_words, emb_dim, num_y, embeds):\n",
    "    super().__init__()\n",
    "    self.emb = nn.Embedding(num_words, emb_dim)\n",
    "    self.emb.weight = nn.Parameter(torch.Tensor(embeds))\n",
    "    self.linear = nn.Linear(emb_dim, num_y)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, text):\n",
    "    embeds = torch.mean(self.emb(text), dim=0)\n",
    "    return self.sigmoid(self.linear(embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AAHeLKUP1yHv"
   },
   "outputs": [],
   "source": [
    "def load_vocab(text):\n",
    "  word_to_ix = {}\n",
    "  for sent , label in text:\n",
    "    for word in sent.split():\n",
    "      word_to_ix.setdefault(word, len(word_to_ix))\n",
    "  return word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7CPELrWCnQW"
   },
   "source": [
    "Define sample train and test sentences with labels (0 neg/1 pos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Br62WAsf2ofF"
   },
   "outputs": [],
   "source": [
    "train_data = [(\"I'm so happy\", 1), (\"I'm so sad\", 0)]\n",
    "tok_to_ix = load_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TznYSVdNC5tY"
   },
   "source": [
    "Create a new model and set the optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "9yHRQWic3i64",
    "outputId": "5af0d658-f1da-4d6d-ce96-d3355626a94f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 300\n",
    "num_classes = 1\n",
    "learning_rate = 0.01\n",
    "embeds = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary = True).wv.vectors\n",
    "model = Net(len(tok_to_ix), emb_dim, num_classes, embeds)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdvjDXyDCylM"
   },
   "source": [
    "Train the model for however many epochs you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "Ztz63Tlx3rVA",
    "outputId": "4c905238-cd44-4392-8df2-42709f579a48"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-91ca60bd59a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "  model.train()\n",
    "  for text, label in train_data:\n",
    "    x = [tok_to_ix[tok] for tok in text.split()]\n",
    "    x_train_tensor = torch.LongTensor(x)\n",
    "    y_train_tensor = torch.Tensor([label])\n",
    "    pred_y = model(x_train_tensor)\n",
    "    loss = loss_fn(pred_y, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "  print(\"\\nEpoch:\", epoch)\n",
    "  print(\"Training loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vd3To31CXE1"
   },
   "source": [
    "Last step! Evaluate the model to see if it predicts the right label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "UwLeSAeUBYEN",
    "outputId": "0d7ebd84-aef2-43e1-8f21-8c0280d09ddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: sad face\n",
      "Predicted label: [0.4775631]\n"
     ]
    }
   ],
   "source": [
    "y_test = 'sad face'\n",
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  x = [tok_to_ix[tok] for tok in y_test.split() if tok in tok_to_ix]\n",
    "  x_test = torch.LongTensor(x)\n",
    "  pred_y_test = model(x_test)\n",
    "\n",
    "  print('Test sentence:', y_test)\n",
    "  print('Predicted label:', pred_y_test.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-demo-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
