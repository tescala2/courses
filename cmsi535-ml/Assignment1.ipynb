{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as skdata\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Name: Escalante, Tomas\n",
    "\n",
    "Collaborators: N/A\n",
    "\n",
    "Collaboration details: N/A\n",
    "\n",
    "Summary:\n",
    "\n",
    "You should answer the questions:\n",
    "1) What did you do in this assignment?\n",
    "   I created a multiclass perceptron and tested its performance on the iris and wine datasets.\n",
    "   \n",
    "2) How did you do it?\n",
    "   I created the multiclass perceptron by using the concepts of binary perceptron. Instead of\n",
    "   using one set of weights, I created three and got values by multiplying each feature vector\n",
    "   by each set of weights and keeping the one with the highest value as the prediction.\n",
    "   \n",
    "3) What are the constants and hyper-parameters you used?\n",
    "   I tested both datasets with tolerances of 1.00, 0.50, 0.10, and 0.01. The iris dataset \n",
    "   produced better results with larger tolerances between 0.50 and 1.00. The wine dataset did \n",
    "   better with smaller tolerances between 0.1 and 0.5. \n",
    "   Rather than initialize the weights as zeros, I made them 0.01, but after comparing the two,\n",
    "   they performed the same.\n",
    "   My training steps didn't seem to effect very much between 10 and 20.\n",
    "\n",
    "Scores:\n",
    "\n",
    "Results on the iris dataset using scikit-learn Perceptron model\n",
    "Training set mean accuracy: 0.6694\n",
    "Validation set mean accuracy: 0.6667\n",
    "Testing set mean accuracy: 0.6429\n",
    "Results on the iris dataset using our Perceptron model trained with 10 steps and tolerance of 1\n",
    "Training set mean accuracy: 0.6694\n",
    "Validation set mean accuracy: 0.6667\n",
    "Results on the iris dataset using our Perceptron model trained with 20 steps and tolerance of 1\n",
    "Training set mean accuracy: 0.6694\n",
    "Validation set mean accuracy: 0.6667\n",
    "Results on the iris dataset using our Perceptron model trained with 60 steps and tolerance of 1\n",
    "Training set mean accuracy: 0.8760\n",
    "Validation set mean accuracy: 0.9333\n",
    "Using best model trained with 60 steps and tolerance of 1\n",
    "Testing set mean accuracy: 0.8571\n",
    "Results on the wine dataset using scikit-learn Perceptron model\n",
    "Training set mean accuracy: 0.5625\n",
    "Validation set mean accuracy: 0.4118\n",
    "Testing set mean accuracy: 0.4706\n",
    "Results on the wine dataset using our Perceptron model trained with 10 steps and tolerance of 0.01\n",
    "Training set mean accuracy: 0.2708\n",
    "Validation set mean accuracy: 0.2353\n",
    "Results on the wine dataset using our Perceptron model trained with 20 steps and tolerance of 0.01\n",
    "Training set mean accuracy: 0.2708\n",
    "Validation set mean accuracy: 0.2353\n",
    "Results on the wine dataset using our Perceptron model trained with 43 steps and tolerance of 0.01\n",
    "Training set mean accuracy: 0.3889\n",
    "Validation set mean accuracy: 0.2941\n",
    "Using best model trained with 43 steps and tolerance of 0.01\n",
    "Testing set mean accuracy: 0.4706\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implementation of Perceptron for multi-class classification\n",
    "'''\n",
    "class PerceptronMultiClass(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Define private variables, weights and number of classes\n",
    "        self.__weights = None\n",
    "        self.__n_class = None\n",
    "\n",
    "    def __update(self, x, y):\n",
    "        '''\n",
    "        Update the weight vector during each training iteration\n",
    "\n",
    "        Args:\n",
    "            x : numpy\n",
    "                d x N feature vector\n",
    "            y : numpy\n",
    "                1 x N ground-truth label\n",
    "        '''\n",
    "        # TODO: Implement the member update function\n",
    "        \n",
    "        # Augment the feature vector x (d, N) with threshold (1, N)\n",
    "        threshold = 0.5 * np.ones([1, x.shape[1]]) # (1, N)\n",
    "        x = np.concatenate([threshold, x], axis = 0) # (d+1, N)\n",
    "        \n",
    "        # Walk through every example and check if they are incorrect\n",
    "        for i in range(x.shape[1]):\n",
    "            # x is (d+1, N), so the shape is (d+1), weights is (c, d+1, 1)\n",
    "            x_i = np.expand_dims(x[:, i], axis = -1) # has shape of (d+1, 1)\n",
    "            \n",
    "            predictions = []\n",
    "            for j in range(len(self.__weights)):\n",
    "                pred = np.matmul(self.__weights[j].T, x_i) # (1, d+1) * (d+1, 1) = (1, 1)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            prediction = np.argmax(predictions)\n",
    "            \n",
    "            # Check if prediction is equal or not equal to ground truth y\n",
    "            if prediction != y[i]:\n",
    "                # w(c_hat)^(t+1) = w(c_hat)^(t) - x^i\n",
    "                # w(star)^(t+1) = w(star)^(t) + x^i\n",
    "                self.__weights[prediction] = self.__weights[prediction] - x_i\n",
    "                self.__weights[y[i]] = self.__weights[y[i]] + x_i\n",
    "                \n",
    "\n",
    "    def fit(self, x, y, T, tol):\n",
    "        '''\n",
    "        Fits the model to x and y by updating the weight vector\n",
    "        based on mis-classified examples for t iterations until convergence\n",
    "\n",
    "        Args:\n",
    "            x : numpy\n",
    "                d x N feature vector\n",
    "            y : numpy\n",
    "                1 x N ground-truth label\n",
    "            T : int\n",
    "                number of iterations to optimize perceptron\n",
    "            tol : float\n",
    "                change of loss tolerance, if greater than loss + tolerance, then stop\n",
    "        '''\n",
    "        # TODO: Implement the fit function\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.__n_class = np.unique(y)\n",
    "        self.__weights = 0.001 * np.ones([len(self.__n_class), x.shape[0] + 1, 1]) # (c, d+1, 1)\n",
    "        self.__weights[:, 0, 0] = -0.001\n",
    "        \n",
    "        \n",
    "        # Initialize previous loss and weights\n",
    "        prev_loss = 2.0\n",
    "        prev_weights = np.copy(self.__weights)\n",
    "        \n",
    "        for t in range(T):\n",
    "            # Compute the loss\n",
    "            predictions = self.predict(x) # (1, N)\n",
    "            \n",
    "            # l = 1/N \\sum_i^N I(h(x^i) != y^i)\n",
    "            loss = np.mean(np.where(predictions != y, 1.0, 0.0))\n",
    "            print('t={} loss={}'.format(t+1, loss))\n",
    "            \n",
    "            # Stopping conditions\n",
    "            if loss == 0.0:\n",
    "                break\n",
    "            elif loss > prev_loss + tol and t > 2:\n",
    "                self.__weights = prev_weights\n",
    "                break\n",
    "            \n",
    "            # Update previous loss and previous weights\n",
    "            prev_loss = loss\n",
    "            prev_weights = np.copy(self.__weights)\n",
    "            \n",
    "            # Updates our weight vector based on what we got wrong\n",
    "            self.__update(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Predicts the label for each feature vector x\n",
    "\n",
    "        Args:\n",
    "            x : numpy\n",
    "                d x N feature vector\n",
    "\n",
    "        Returns:\n",
    "            numpy : 1 x N label vector\n",
    "        '''\n",
    "        # TODO: Implement the predict function\n",
    "        \n",
    "        # [w0, w1, w2, w3, ..., wd] (d+1, N)\n",
    "        # [..., x1, x2, x3, ..., xd] (d, N)\n",
    "        # What is the shape of threshold? (1, N)\n",
    "        \n",
    "        threshold = 0.5 * np.ones([1, x.shape[1]]) # (1, N)\n",
    "        \n",
    "        # Augment the the features x with the threshold\n",
    "        x = np.concatenate([threshold, x], axis = 0) # (d+1, N)\n",
    "        \n",
    "        # Predict using w^Tx: \n",
    "        # Predict the label for x_i\n",
    "        all_predictions = []\n",
    "        for i in range(x.shape[1]):\n",
    "            # x is (d+1, N), so the shape is (d+1), weights is (c, d+1, 1)\n",
    "            x_i = np.expand_dims(x[:, i], axis = -1) # has shape of (d+1, 1)\n",
    "            \n",
    "            predictions = []\n",
    "            for j in range(len(self.__weights)):\n",
    "                pred = np.matmul(self.__weights[j].T, x_i) # (1, d+1) * (d+1, 1) = (1, 1)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            prediction = np.argmax(predictions)\n",
    "            all_predictions.append(prediction)\n",
    "            \n",
    "        return all_predictions # (1, N)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Predicts labels based on feature vector x and computes the mean accuracy\n",
    "        of the predictions\n",
    "\n",
    "        Args:\n",
    "            x : numpy\n",
    "                d x N feature vector\n",
    "            y : numpy\n",
    "                1 x N ground-truth label\n",
    "\n",
    "        Returns:\n",
    "            float : mean accuracy\n",
    "        '''\n",
    "        # TODO: Implement the score function\n",
    "        \n",
    "        predictions = self.predict(x) # (1, N) of {0, 1, 2}\n",
    "        \n",
    "        # Comparing if predictions and y are the same\n",
    "        scores = np.where(predictions == y, 1.0, 0.0)\n",
    "        \n",
    "        # Return the mean accuracy\n",
    "        return np.mean(scores)\n",
    "\n",
    "\n",
    "def split_dataset(x, y, n_sample_train_to_val_test=8):\n",
    "    '''\n",
    "    Helper function to splits dataset into training, validation and testing sets\n",
    "\n",
    "    Args:\n",
    "        x : numpy\n",
    "            d x N feature vector\n",
    "        y : numpy\n",
    "            1 x N ground-truth label\n",
    "        n_sample_train_to_val_test : int\n",
    "            number of training samples for every validation, testing sample\n",
    "\n",
    "    Returns:\n",
    "        x_train : numpy\n",
    "            d x n feature vector\n",
    "        y_train : numpy\n",
    "            1 x n ground-truth label\n",
    "        x_val : numpy\n",
    "            d x m feature vector\n",
    "        y_val : numpy\n",
    "            1 x m ground-truth label\n",
    "        x_test : numpy\n",
    "            d x m feature vector\n",
    "        y_test : numpy\n",
    "            1 x m ground-truth label\n",
    "    '''\n",
    "    n_sample_interval = n_sample_train_to_val_test + 2\n",
    "\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    for idx in range(x.shape[0]):\n",
    "        if idx and idx % n_sample_interval == (n_sample_interval - 1):\n",
    "            val_idx.append(idx)\n",
    "        elif idx and idx % n_sample_interval == 0:\n",
    "            test_idx.append(idx)\n",
    "        else:\n",
    "            train_idx.append(idx)\n",
    "\n",
    "    x_train, x_val, x_test = x[train_idx, :], x[val_idx, :], x[test_idx, :]\n",
    "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    iris_data = skdata.load_iris()\n",
    "    wine_data = skdata.load_wine()\n",
    "\n",
    "    datasets = [iris_data, wine_data]\n",
    "    tags = ['iris', 'wine']\n",
    "\n",
    "    # TODO: Experiment with 3 different max training steps (T) for each dataset\n",
    "    train_steps_iris = [10, 20, 48]\n",
    "    train_steps_wine = [10, 20, 50]\n",
    "\n",
    "    train_steps = [train_steps_iris, train_steps_wine]\n",
    "\n",
    "    # TODO: Set a tolerance for each dataset\n",
    "    tol_iris = 1\n",
    "    tol_wine = 0.5\n",
    "\n",
    "    tols = [tol_iris, tol_wine]\n",
    "\n",
    "    for dataset, steps, tol, tag in zip(datasets, train_steps, tols, tags):\n",
    "        # Split dataset into 80 training, 10 validation, 10 testing\n",
    "        x = dataset.data\n",
    "        y = dataset.target\n",
    "        x_train, y_train, x_val, y_val, x_test, y_test = split_dataset(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            n_sample_train_to_val_test=8)\n",
    "\n",
    "        '''\n",
    "        Trains and tests Perceptron model from scikit-learn\n",
    "        '''\n",
    "        model = Perceptron(penalty=None, alpha=0.0, tol=tol)\n",
    "        # Trains scikit-learn Perceptron model\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        print('Results on the {} dataset using scikit-learn Perceptron model'.format(tag))\n",
    "\n",
    "        # Test model on training set\n",
    "        scores_train = model.score(x_train, y_train)\n",
    "        print('Training set mean accuracy: {:.4f}'.format(scores_train))\n",
    "\n",
    "        # Test model on validation set\n",
    "        scores_val = model.score(x_val, y_val)\n",
    "        print('Validation set mean accuracy: {:.4f}'.format(scores_val))\n",
    "\n",
    "        # Test model on testing set\n",
    "        scores_test = model.score(x_test, y_test)\n",
    "        print('Testing set mean accuracy: {:.4f}'.format(scores_test))\n",
    "\n",
    "        '''\n",
    "        Trains, validates, and tests our Perceptron model for multi-class classification\n",
    "        '''\n",
    "        # TODO: obtain dataset in correct shape (d x N)\n",
    "        x_train = np.transpose(x_train, axes=(1, 0))\n",
    "        x_val = np.transpose(x_val, axes=(1, 0))\n",
    "        x_test = np.transpose(x_test, axes=(1, 0))\n",
    "\n",
    "        # Initialize empty lists to hold models and scores\n",
    "        models = []\n",
    "        scores = []\n",
    "        for T in steps:\n",
    "            # TODO: Initialize PerceptronMultiClass model\n",
    "            PMC_model = PerceptronMultiClass()\n",
    "\n",
    "            print('Results on the {} dataset using our Perceptron model trained with {} steps and tolerance of {}'.format(tag, T, tol))\n",
    "            # TODO: Train model on training set\n",
    "            PMC_model.fit(x_train, y_train, T=T, tol=tol)\n",
    "\n",
    "            # TODO: Test model on training set\n",
    "            scores_train = PMC_model.score(x_train, y_train)\n",
    "            print('Training set mean accuracy: {:.4f}'.format(scores_train))\n",
    "\n",
    "            # TODO: Test model on validation set\n",
    "            scores_val = PMC_model.score(x_val, y_val)\n",
    "            print('Validation set mean accuracy: {:.4f}'.format(scores_val))\n",
    "\n",
    "            # TODO: Save the model and its score\n",
    "            models.append(PMC_model)\n",
    "            scores.append(scores_val)\n",
    "\n",
    "\n",
    "        # TODO: Select the best performing model on the validation set\n",
    "        best_idx = np.argmax(scores)\n",
    "\n",
    "        print('Using best model trained with {} steps and tolerance of {}'.format(steps[best_idx], tol))\n",
    "\n",
    "        # TODO: Test model on testing set\n",
    "        scores_test = models[best_idx].score(x_test, y_test)\n",
    "        print('Testing set mean accuracy: {:.4f}'.format(scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
